{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick Start With N-MNIST \n",
    "\n",
    "This tutorial explains all steps necessary to deploy a pretrained SNN to the devkit.\n",
    "\n",
    "1. The pretrained SNN will be obtained by:\n",
    "   -  CNN-to-SNN conversion.\n",
    "   -  Train SNN with BPTT(Back Propagation Through Time)\n",
    "\n",
    "2. As for the dataset, we use the [\"N-MNIST\"](https://www.garrickorchard.com/datasets/n-mnist) dataset.The Neuromorphic-MNIST (N-MNIST) dataset is a spiking version of the original frame-based MNIST dataset. It consists of the same 60 000 training and 10 000 testing samples with a resolution of 34*34 pixels. [\"Tonic\"](https://tonic.readthedocs.io/en/latest/index.html) provides publicly available event-based vision and audio datasets and event transformations. The package is fully compatible with PyTorch. Thus we will use Tonic as the tool for data preparation.\n",
    "\n",
    "3. In deployment stage, we use an auxiliary class [`DynapcnnNetwork`](https://gitlab.com/synsense/sinabs-dynapcnn/-/blob/master/sinabs/backend/dynapcnn/dynapcnn_network.py) to convert the pretrained SNN to a `configuration` object of the devkit.\n",
    "\n",
    "4. Further more, we take a in-depth investigation of the deployment process and reveal more details about the chip. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "except ImportError:\n",
    "    ! pip install tonic\n",
    "    from tonic.datasets.nmnist import NMNIST\n",
    "    \n",
    "# download dataset\n",
    "root_dir = \"./NMNIST\"\n",
    "_ = NMNIST(save_to=root_dir, train=True)\n",
    "_ = NMNIST(save_to=root_dir, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type of data is: <class 'numpy.ndarray'>\n",
      "time length of sample data is: 300760 micro seconds\n",
      "there are 4686 events in the sample data\n",
      "the label of the sample data is: 5\n"
     ]
    }
   ],
   "source": [
    "sample_data, label = NMNIST(save_to=root_dir, train=False)[0]\n",
    "\n",
    "print(f\"type of data is: {type(sample_data)}\")\n",
    "print(f\"time length of sample data is: {sample_data['t'][-1] - sample_data['t'][0]} micro seconds\")\n",
    "print(f\"there are {len(sample_data)} events in the sample data\")\n",
    "print(f\"the label of the sample data is: {label}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-To-SNN\n",
    "\n",
    "Tips for model \"devkit-friendly architecture\" can be found at [here.](../faqs/available_network_arch.md) \n",
    "\n",
    "Layer types supported by the devkit can be found at [here.](../faqs/available_algorithmic_operation.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "# define a CNN model\n",
    "cnn = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    nn.ReLU(),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    nn.ReLU(),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in cnn.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice that the output layer is a `ReLU` instead of a `Linear` directly. This is because, when converting to SNN, we directly replace the `ReLU` non-linear activation layer with an `IAF` layer. The conversion function need a `IAF` layer as the output layer. Also, at the devkit inference stage,  the output is spikes. So using `IAF` as the output layer is also a choice that is more in line with our hardware operation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (1, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "from tonic.transforms import ToFrame\n",
    "from tonic.datasets import nmnist\n",
    "\n",
    "# define a transform that accumulate the events into a single frame image\n",
    "to_frame = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=1)\n",
    "\n",
    "cnn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_frame)\n",
    "cnn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_frame)\n",
    "\n",
    "# check the transformed data\n",
    "sample_data, label = cnn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test CNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b4e1bebae7949f09dbe4e464f1639b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edd62b44d6b54713b599960db185701c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - accuracy: 55.55%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7922436dc84d4f52b6f199a04e61690c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3a17647211a4cb4bbe157e4ad513191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - accuracy: 63.949999999999996%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d58f8708b5043aeab037426f3584963",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e09372beb2a0458882f304c29a6868c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 - accuracy: 74.72999999999999%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "epochs = 3\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda:0\"\n",
    "shuffle = True\n",
    "\n",
    "cnn = cnn.to(device=device)\n",
    "\n",
    "cnn_train_dataloader = DataLoader(cnn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "cnn_test_dataloader = DataLoader(cnn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=shuffle)\n",
    "\n",
    "optimizer = SGD(params=cnn.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(cnn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # remove the time-step axis since we are training CNN\n",
    "        # move the data to accelerator\n",
    "        data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = cnn(data)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(cnn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # remove the time-step axis since we are training CNN\n",
    "            # move the data to accelerator\n",
    "            data = data.squeeze(dim=1).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = cnn(data)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert CNN To SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sinabs.from_torch import from_model\n",
    "\n",
    "snn_convert = from_model(model=cnn, input_shape=(2, 34, 34), batch_size=batch_size).spiking_model\n",
    "snn_convert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Converted SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac69880542c241968f69abd8a6f9a317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of converted SNN: 70.47%\n"
     ]
    }
   ],
   "source": [
    "# define a transform that accumulate the events into a raster-like tensor\n",
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_convert = snn_convert.to(device)\n",
    "\n",
    "correct_predictions = []\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader)\n",
    "    for data, label in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        output = snn_convert(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "        # set progressing bar\n",
    "        test_p_bar.set_description(f\"Testing SNN Model...\")\n",
    "\n",
    "    correct_predictions = torch.cat(correct_predictions)\n",
    "    print(f\"accuracy of converted SNN: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degraded Performance After Conversion\n",
    "\n",
    "You might observe a degraded performance after the CNN-to-SNN conversion which might caused by: \n",
    "\n",
    "- IAF neuron stay silent after converting to SNN. \n",
    "- the distribution mis-match between the output of ReLU activation layer and the output of spiking IAF layer.\n",
    "- Difference between synchronous-convolution and asynchronous-convolution.\n",
    "\n",
    "To mitigate this issue, we usually apply the following tricks on the CNN/SNN's:\n",
    "\n",
    "1. Re-scaling the first parameter layer's weights of the SNN which prevent the SNN from non-spking,\n",
    "2. If trick No.1 not work well, in `sinabs` we provide an auxiliary function `sinabs.utils.normalize_weights` which can help to normalize the activation of the CNN. More details can be found [here.](https://sinabs.readthedocs.io/en/1.2.8/api/utils.html#sinabs.utils.normalize_weights)\n",
    "\n",
    "If the tricks above do not help, try to train an SNN directly with BPTT. We here only focus on how to use the devkit instead of optimizing the performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SNN with BPTT\n",
    "\n",
    "Instead of using a CNN-to-SNN conversion, we are able to train an SNN directly with BPTT. `sinabs-exodus` provides a CUDA enhanced IAF neuron layer, which helps to speed-up the training process of the SNN. Here we recommend to use layers from the `sinabs-exodus` for faster training speed. The **installation** of the `sinabs-exodus` can be found [here.](https://github.com/synsense/sinabs-exodus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sinabs.layers as sl\n",
    "from torch import nn\n",
    "from sinabs.activation.surrogate_gradient_fn import PeriodicExponential\n",
    "\n",
    "# just replace the ReLU layer with the sl.IAFSqueeze\n",
    "snn_bptt = nn.Sequential(\n",
    "    # [2, 34, 34] -> [8, 17, 17]\n",
    "    nn.Conv2d(in_channels=2, out_channels=8, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [8, 17, 17] -> [16, 8, 8]\n",
    "    nn.Conv2d(in_channels=8, out_channels=16, kernel_size=(3, 3), padding=(1, 1), bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    # [16 * 8 * 8] -> [16, 4, 4]\n",
    "    nn.Conv2d(in_channels=16, out_channels=16, kernel_size=(3, 3), padding=(1, 1), stride=(2, 2),  bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    "    # [16 * 4 * 4] -> [10]\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(16 * 4 * 4, 10, bias=False),\n",
    "    sl.IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, surrogate_grad_fn=PeriodicExponential()),\n",
    ")\n",
    "\n",
    "# init the model weights\n",
    "for layer in snn_bptt.modules():\n",
    "    if isinstance(layer, (nn.Conv2d, nn.Linear)):\n",
    "        nn.init.xavier_normal_(layer.weight.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Disable All \"Bias\" Of The Convolutional Layers?\n",
    "\n",
    "The bias term in fact is related to the neuron's leak mechanism on the hardware. The speed of the neuron v_mem leakage is effected by an external slow-clock.\n",
    "\n",
    "More details of the `bias` and `neuron leak` can be found in the following 2 docs:\n",
    "\n",
    "1. [Bias is a no no.](../dangers.md)\n",
    "2. [How to leak the neuron.](./leak_neuron.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covert To Exodus Model If Exodus Available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): EXODUS IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): EXODUS IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): EXODUS IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): EXODUS IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from sinabs.exodus import conversion\n",
    "    snn_bptt = conversion.sinabs_to_exodus(snn_bptt)\n",
    "except ImportError:\n",
    "    print(\"Sinabs-exodus is not intalled.\")\n",
    "\n",
    "snn_bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define SNN Training & Testing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_time_steps = 100\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=n_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to=root_dir, train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False, transform=to_raster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Test SNN With BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c4fe28dad1e4deca6605e895e55592f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "480796f3a8be42ed956fd37295375e6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - BPTT accuracy: 92.34%\n"
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "lr = 1e-3\n",
    "batch_size = 4\n",
    "num_workers = 4\n",
    "device = \"cuda:0\"\n",
    "shuffle = True\n",
    "\n",
    "snn_train_dataloader = DataLoader(snn_train_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_dataset, batch_size=batch_size, num_workers=num_workers, drop_last=True, shuffle=False)\n",
    "\n",
    "snn_bptt = snn_bptt.to(device=device)\n",
    "\n",
    "optimizer = SGD(params=snn_bptt.parameters(), lr=lr)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "for e in range(epochs):\n",
    "\n",
    "    # train\n",
    "    train_p_bar = tqdm(snn_train_dataloader)\n",
    "    for data, label in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "        label = label.to(dtype=torch.long, device=device)\n",
    "        # forward\n",
    "        optimizer.zero_grad()\n",
    "        output = snn_bptt(data)\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, n_time_steps, -1)\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "        loss = criterion(output, label)\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        for layer in snn_bptt.modules():\n",
    "            if isinstance(layer, sl.StatefulLayer):\n",
    "                for name, buffer in layer.named_buffers():\n",
    "                    buffer.detach_()\n",
    "        \n",
    "        # set progressing bar\n",
    "        train_p_bar.set_description(f\"Epoch {e} - BPTT Training Loss: {round(loss.item(), 4)}\")\n",
    "\n",
    "    # validate\n",
    "    correct_predictions = []\n",
    "    with torch.no_grad():\n",
    "        test_p_bar = tqdm(snn_test_dataloader)\n",
    "        for data, label in test_p_bar:\n",
    "            # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "            data = data.reshape(-1, 2, 34, 34).to(dtype=torch.float, device=device)\n",
    "            label = label.to(dtype=torch.long, device=device)\n",
    "            # forward\n",
    "            output = snn_bptt(data)\n",
    "            # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "            output = output.reshape(batch_size, n_time_steps, -1)\n",
    "            # accumulate all time-steps output for final prediction\n",
    "            output = output.sum(dim=1)\n",
    "            # calculate accuracy\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            # compute the total correct predictions\n",
    "            correct_predictions.append(pred.eq(label.view_as(pred)))\n",
    "            # set progressing bar\n",
    "            test_p_bar.set_description(f\"Epoch {e} - BPTT Testing Model...\")\n",
    "    \n",
    "        correct_predictions = torch.cat(correct_predictions)\n",
    "        print(f\"Epoch {e} - BPTT accuracy: {correct_predictions.sum().item()/(len(correct_predictions))*100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain a SNN with better on-chip performance, please refer to the [training tips.](../faqs/tips_for_training.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Back To Sinabs Model If Using Exodus Model For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(2, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (1): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (4): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (6): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (7): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       "  (8): Flatten(start_dim=1, end_dim=-1)\n",
       "  (9): Linear(in_features=256, out_features=10, bias=False)\n",
       "  (10): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1.0, batch_size=4, num_timesteps=-1)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    from sinabs.exodus import conversion\n",
    "    snn_bptt = conversion.exodus_to_sinabs(snn_bptt)\n",
    "except ImportError:\n",
    "    print(\"Sinabs-exodus is not intalled.\")\n",
    "\n",
    "snn_bptt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy SNN To The Devkit\n",
    "\n",
    "To deploy the SNN to the devkit, we use an auxiliary class `DynapcnnNetwork` to convert the pretrained SNN to a configuration object of the devkit.\n",
    "\n",
    "In the example beblow, we use the \"Speck2fModuleDevKit\" as the inference device. More devkit names can be found [here.](https://gitlab.com/synsense/sinabs-dynapcnn/-/blob/master/sinabs/backend/dynapcnn/chip_factory.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n",
      "The SNN is deployed on the core: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "\n",
    "# cpu_snn = snn_convert.to(device=\"cpu\")\n",
    "cpu_snn = snn_bptt.to(device=\"cpu\")\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=False)\n",
    "devkit_name = \"speck2fmodule\"\n",
    "\n",
    "# use the `to` method of DynapcnnNetwork to deploy the SNN to the devkit\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=\"auto\")\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "press the \"reset button\" on the hardware and re-run the code block above if you meet a \"time-out error\"!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference On The Devkit\n",
    "\n",
    "Our devkits takes [`samna events`](https://synsense-sys-int.gitlab.io/samna/reference/speck2f/event/index.html) stream as the input. So before sending the raw N-MNIST events into devkit, we need first convert the data to samna's [`samna.speck2f.event.Spike`](https://synsense-sys-int.gitlab.io/samna/reference/speck2f/event/index.html#samna.speck2f.event.Spike) events stream.\n",
    "\n",
    "**Notice**: Different types of devkit need different types of event as its input. For example, if you're using a `DynapcnnDevKit`, you need to use the [`samna.dynapcnn.event.Spike`](https://synsense-sys-int.gitlab.io/samna/reference/dynapcnn/event/index.html#samna.dynapcnn.event.Spike) as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e0165b5957a441bbe5a2a7e865f52cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On chip inference accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "import samna\n",
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "test_samples = 0\n",
    "correct_samples = 0\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # create samna Spike events stream\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        spk = samna.speck2f.event.Spike()\n",
    "        spk.x = ev['x']\n",
    "        spk.y = ev['y']\n",
    "        spk.timestamp = ev['t'] - events['t'][0]\n",
    "        spk.feature = ev['p']\n",
    "        # Spikes will be sent to layer/core #0, since the SNN is deployed on core: [0, 1, 2, 3]\n",
    "        spk.layer = 0\n",
    "        samna_event_stream.append(spk)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but each Spike.layer is 3, since layer#3 is the output layer\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    neuron_index = [each.feature for each in output_events]\n",
    "    if len(neuron_index) != 0:\n",
    "        frequent_counter = Counter(neuron_index)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label}, prediction: {prediction}， output spikes num: {len(output_events)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-depth Investigation Of SNN Deployment Stage\n",
    "\n",
    "Basicly, the key factors for deploying the SNN to the devkit and interacting with it are the following 2:\n",
    "\n",
    "- **hardware configuration**. It contains the SNN's **quantized weights**, spking thresholds and connectivity etc. See more details about the devkit configuration in [samna's doc.](https://synsense-sys-int.gitlab.io/samna/reference/speck2f/configuration/index.html)\n",
    "- **samna graph**. It defines how input and output event streams flows beween the devkit and the host machine. See more details about the samna graph in [samna's doc.](https://synsense-sys-int.gitlab.io/samna/filters.html#eventfiltergraph)\n",
    "\n",
    "In the example above, `DynapcnnNetwork`'s `.to` method implicitly calls a `.make_config` method to build a \"hardware configuration\" object for the devkit and then applys the configuration to the devkit before the calling of `.to` is finished.\n",
    "\n",
    "Apart from that, when `.to` is called, a simple samna graph which supports the basic input-writing and output-reading for the devkit is built.\n",
    "\n",
    "\n",
    "### How SNN Is Deployed To The Processor?\n",
    "\n",
    "\n",
    "\n",
    "The \"hardware configuration\" has an attribute called `.cnn_layers` which contains the SNN quantized weights and connectivity. The weights of the SNN is quantized into a `int8` precision and the \"membrane potential(v_mem, neuron states)\" of the IAF neuron is quantized into a `int16` precision.\n",
    "\n",
    "The `nn.Linear` layers in the model will be converted to a equivalent `nn.Conv2d` and the `nn.AvgPool2d` layers will also be converted to a `sinabs.layers.SumPool2d` before generating the hardware configuration. More details can be found in the [FAQs.](../faqs/available_algorithmic_operation.md)\n",
    "\n",
    "Current version of `sinabs-dynpacnn` only supports parsing the `nn.Sequential` like architechture. In the future version, a feature of network graph extraction will be integrated. If you would like to try more complex architectures like a \"residual-connection\", please refer to the [FAQs.](../faqs/available_network_arch.md)\n",
    "\n",
    "The SNN is deployed to the devkit once the \"hardware configuration\" is applied to the devkit. Before the calling of `.to` method is finished, it implicitly applys the configutation by executing `self.samna_device.get_model().apply_configuration(config)`.\n",
    "\n",
    "\n",
    "### How Input Data Is Sent To The Processor?\n",
    "\n",
    "![input_data_flow](/_static/nmnist_quick_start/spike_input_flow.png)\n",
    "\n",
    "As stated above, **samna graph** controls how data flows between the devkit and host machine. The samna graph usually has a \"input buffer node\" for receiving input from the host machine. After preparing a list of \"samna events\"(Spike, DvsEvent and ReadNeuronValue etc.), we call the `.write` method of the \"input buffer node\", like `input_buffer.write(events_list)` and the entire list of \"samna events\" will be sent to the devkit.\n",
    "\n",
    "Apart from that, the devkit has a **stopwatch**(in the FPGA) which is timed in **micro-seconds** unit. Only When the time of the stopwatch is **greater than or equal to** the `timestamp` of the input event, the event will be sent into the DynapCNN layer or DVS layer for processing. In `DynapcnnNetwork`'s `.forward` method, everytime before the \"samna events\" is written into the devkit, it **restarts the \"stopwatch\"** automatically. **So we usually shift the input events' timestamps to let the timestamps start from 0.**\n",
    "\n",
    "If the current time of the stopwatch is already greater than all timestamps of the input events, all events will be sent for processing **immediately!**\n",
    "\n",
    "\n",
    "### How Output Data Is Read From The Devkit?\n",
    "\n",
    "Similarly, the samna graph also has an \"output buffer node\" which supports users read output events from the devkit. By calling the `.get_events` method of the \"output buffer node\", users can obtain the output events as a list: `output_list = output_buffer.get_events()`. In `DynapcnnNetwork`'s `.forward` method it implicitly calls the `output_buffer.get_events()` and get the output.\n",
    "\n",
    "**It is possible to set multiple output buffer nodes for one samna graph.** By setting different types of [\"Filter Node\"](https://synsense-sys-int.gitlab.io/samna/filters.html#filter-nodes) before each output buffer node, users can read different types of output events from different output buffer more efficiently.\n",
    "\n",
    "\n",
    "### An Adanved Example With Visualizer\n",
    "\n",
    "In the example code below, we show that how to make a further modification on the \"hardware configurration\" to exploit more features of the devkit like:\n",
    "\n",
    "- monitor the input events\n",
    "- monitor the hidden layers' output spikes\n",
    "\n",
    "Besides, we use a `Visualizer` to show the input event streams on a GUI window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/allan/newssd/synsense_codes/sinabs-dynapcnn/dynapcnn_venv/lib/python3.8/site-packages/sinabs/backend/dynapcnn/chips/dynapcnn.py:289: UserWarning: Layer 0 has pooling and is being monitored. Note that pooling will not be reflected in the monitored events.\n",
      "  warn(\n",
      "/home/allan/newssd/synsense_codes/sinabs-dynapcnn/dynapcnn_venv/lib/python3.8/site-packages/sinabs/backend/dynapcnn/chips/dynapcnn.py:289: UserWarning: Layer 1 has pooling and is being monitored. Note that pooling will not be reflected in the monitored events.\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The SNN is deployed on the core: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "import samnawe\n",
    "# first define a callback function to modify the devkit configuration\n",
    "# the callback function should only has 1 devkit config instance as its input argument\n",
    "def config_modify_callback(devkit_cfg):\n",
    "\n",
    "    # enable visualizing the output from dvs(pre-processing) layer\n",
    "    devkit_cfg.dvs_layer.monitor_enable = True\n",
    "    # disable visualizing the events generated by the embedded dvs on Speck\n",
    "    devkit_cfg.dvs_layer.raw_monitor_enable = False\n",
    "    # prevent the events generated by the embedded dvs been feed to the DynapCNN Core.\n",
    "    devkit_cfg.dvs_layer.pass_sensor_events = False\n",
    "    # point the dvs layer output destination to the core#0 \n",
    "    devkit_cfg.dvs_layer.destinations[0].enable = True\n",
    "    devkit_cfg.dvs_layer.destinations[0].layer = 0\n",
    "\n",
    "    # the callback must return the modified devkit config\n",
    "    return devkit_cfg\n",
    "\n",
    "# close the devkit before reopen\n",
    "samna.device.close_device(dynapcnn.samna_device)\n",
    "\n",
    "# init DynapcnnNetwork instance\n",
    "dynapcnn = DynapcnnNetwork(snn=cpu_snn, input_shape=(2, 34, 34), discretize=True, dvs_input=True)\n",
    "\n",
    "devkit_name = \"speck2fmodule\"\n",
    "# define which layers output you want to monitor\n",
    "layers_to_monitor = [0, 1, 2, 3]\n",
    "# pass the callback function into the `.to` method\n",
    "dynapcnn.to(device=devkit_name, chip_layers_ordering=[0, 1, 2, 3], monitor_layers=layers_to_monitor, config_modifier=config_modify_callback)\n",
    "print(f\"The SNN is deployed on the core: {dynapcnn.chip_layers_ordering}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of pass a list of layer indices to the `monitor_layers` to enable reading output from the specific layer, you can also achieve that goal by adding some codes in the `config_modify_callback`:\n",
    "```python\n",
    "def config_modify_callback(devkit_cfg):\n",
    "\n",
    "    # enable visualizing the output from dvs(pre-processing) layer\n",
    "    devkit_cfg.dvs_layer.monitor_enable = True\n",
    "    # disable visualizing the events generated by the embedded dvs\n",
    "    devkit_cfg.dvs_layer.raw_monitor_enable = False\n",
    "    # prevent the events generated by the embedded dvs been feed to the DynapCNN Core.\n",
    "    devkit_cfg.dvs_layer.pass_sensor_events = False\n",
    "    # point the dvs layer output destination to the core#0 \n",
    "    devkit_cfg.dvs_layer.destinations[0].enable = True\n",
    "    devkit_cfg.dvs_layer.destinations[0].layer = 0\n",
    "\n",
    "    # **enable monitoring all layers output**\n",
    "    for layer in [0, 1, 2, 3]:\n",
    "        devkit_cfg.cnn_layers[layer].monitor_enable = True\n",
    "\n",
    "    # the callback must return the modified devkit config\n",
    "    return devkit_cfg\n",
    "```\n",
    "\n",
    "What I want to say here is that in most cases, **exploit different features of the devkit is essentially modifying the devkit configuration of the devkit.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use DynapcnnVisualizer\n",
    "After the SNN is deployed to the devkit. We can use the [`DynapcnnVisualizer`](../visualizer.md) to visualize the input events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting: Please wait until the JIT compilation is done, this might take a while. You will get notified on completion.\n",
      "Set up completed!\n"
     ]
    }
   ],
   "source": [
    "from sinabs.backend.dynapcnn.dynapcnn_visualizer import DynapcnnVisualizer\n",
    "\n",
    "\n",
    "visualizer = DynapcnnVisualizer(\n",
    "    window_scale=(4, 8),\n",
    "    dvs_shape=(34, 34),\n",
    "    spike_collection_interval=50,\n",
    ")\n",
    "\n",
    "visualizer.connect(dynapcnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the visualizer is built, **you should see a GUI window pop out**.\n",
    "\n",
    "![visualizer](/_static/nmnist_quick_start/dynapcnn_visualizer.png)\n",
    "\n",
    "By running the code block below, the input events will be displayed on the GUI window.\n",
    "\n",
    "We now start to write inputs to the devkit.In the example above, we use the `samna.speck2f.event.Spike` as the input event type. Since now we need to visualize the input events which we write into the devkit, we need to use the `samna.speck2f.event.DvsEvent`. It is because only by this the input events can be captured by the visualizer. More details about the \"DvsEvent\" can be found [here.](https://synsense-sys-int.gitlab.io/samna/reference/speck2f/event/index.html#samna.speck2f.event.DvsEvent) \n",
    "\n",
    "![dvs_event_input_flow](/_static/nmnist_quick_start/dvs_input_flow.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4aec4aa38b844ceb626758febdab299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On chip inference accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "snn_test_dataset = NMNIST(save_to=root_dir, train=False)\n",
    "# for time-saving, we only select a subset for on-chip infernce， here we select 1/100 for an example run\n",
    "subset_indices = list(range(0, len(snn_test_dataset), 100))\n",
    "snn_test_dataset = Subset(snn_test_dataset, subset_indices)\n",
    "\n",
    "inferece_p_bar = tqdm(snn_test_dataset)\n",
    "\n",
    "for events, label in inferece_p_bar:\n",
    "\n",
    "    # instead of creating Spike and send it to core#0 directly, we now create DvsEvent(for visualization) and send it to the DVS layer\n",
    "    # since in the \"config_modify_callback\" we point the output destination layer of the DVS layer to layer/core #0\n",
    "    # so the DynacnnCore can still receive the same input as before.\n",
    "    samna_event_stream = []\n",
    "    for ev in events:\n",
    "        dvs_ev = samna.speck2f.event.DvsEvent()\n",
    "        dvs_ev.x = ev['x']\n",
    "        dvs_ev.y = ev['y']\n",
    "        dvs_ev.timestamp = ev['t'] - events['t'][0]\n",
    "        dvs_ev.p = ev['p']\n",
    "        samna_event_stream.append(dvs_ev)\n",
    "\n",
    "    # inference on chip\n",
    "    # output_events is also a list of Spike, but .layer will have 0, 1, 2, 3 since we choose to monitor all layers' output\n",
    "    output_events = dynapcnn(samna_event_stream)\n",
    "    \n",
    "    # get each layers output spikes\n",
    "    layer0_spks = [each.feature for each in output_events if each.layer == 0]\n",
    "    layer1_spks = [each.feature for each in output_events if each.layer == 1]\n",
    "    layer2_spks = [each.feature for each in output_events if each.layer == 2]\n",
    "    layer3_spks = [each.feature for each in output_events if each.layer == 3]\n",
    "    # use the most frequent output neruon index as the final prediction\n",
    "    if len(layer3_spks) != 0:\n",
    "        frequent_counter = Counter(layer3_spks)\n",
    "        prediction = frequent_counter.most_common(1)[0][0]\n",
    "    else:\n",
    "        prediction = -1\n",
    "    inferece_p_bar.set_description(f\"label: {label} prediction: {prediction}，layer 0 output spks: {len(layer0_spks)},layer 1 output spikes num: {len(layer1_spks)}, layer 2 output spikes num: {len(layer2_spks)},layer 3 output spikes num: {len(layer3_spks)}\") \n",
    "\n",
    "    if prediction == label:\n",
    "        correct_samples += 1\n",
    "\n",
    "    test_samples += 1\n",
    "    \n",
    "print(f\"On chip inference accuracy: {correct_samples / test_samples}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Success. You have completed all steps!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
