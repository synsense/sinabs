{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From a CNN model to a DYNAP-CNN DevKit\n",
    "\n",
    "This tutorial explains all steps necessary to convert a torch CNN model to a configuration of the DYNAP-CNN chip. We will first convert the network to a spiking neural network (SNN) and then to a `DynapcnnNetwork` – a model that is compatible with the chip and simulates its behavior. Finally we port the model to a DYNAP-CNN DevKit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "\n",
    "Before we start, we will import the libraries necessary to define a torch model, convert it to an SNN and to convert the SNN to a `DynapcnnNetwork`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "# Suppress warnings (This is only to keep the notebook pretty. You might want to comment the below two lines)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# - Import statements\n",
    "import torch\n",
    "import samna\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets\n",
    "import sinabs\n",
    "from sinabs.from_torch import from_model\n",
    "from sinabs.backend.dynapcnn import io\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from sinabs.backend.dynapcnn.chip_factory import ChipFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN definition\n",
    "\n",
    "First we will define a sequential CNN model.\n",
    "\n",
    "*Note that although non-sequential models are supported by the hardware, this is not yet the case for this library.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# - Define CNN model\n",
    "\n",
    "ann = nn.Sequential(\n",
    "    nn.Conv2d(1, 20, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(20, 32, 5, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Conv2d(32, 128, 3, 1, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.AvgPool2d(2, 2),\n",
    "    nn.Flatten(),\n",
    "    nn.Linear(128, 500, bias=False),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(500, 10, bias=False),\n",
    ")\n",
    "\n",
    "# Load pre-trained weights\n",
    "ann.load_state_dict(torch.load(\"../../../examples/mnist_params.pt\", map_location=\"cpu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation\n",
    "\n",
    "Lets pass some data and see how this converted spiking model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom dataset for spiking input data\n",
    "class MNIST_Dataset(datasets.MNIST):\n",
    "\n",
    "    def __init__(self, root, train = True, spiking=False, tWindow=100):\n",
    "        super().__init__(root, train=train, download=True)\n",
    "        self.spiking=spiking\n",
    "        self.tWindow = tWindow\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.data[index], self.targets[index]\n",
    "\n",
    "        if self.spiking:\n",
    "            img = (np.random.rand(self.tWindow, 1, *img.size()) < img.numpy()/255.0).astype(float)\n",
    "            img = torch.from_numpy(img).float()\n",
    "        else:\n",
    "            # Convert image to tensor\n",
    "            img = torch.from_numpy(img.numpy()).float()\n",
    "            img.unsqueeze_(0)\n",
    "\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "# Define dataloader\n",
    "tWindow = 200 # ms (or) time steps\n",
    "\n",
    "# Define test dataset\n",
    "test_dataset = MNIST_Dataset(\"./data\", train=False, spiking=True, tWindow=tWindow)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scale the parameters in each layer to as to normalise the output activation to a certain percentile. This helps performance on the chip a lot because output is so heavily quantized. More information about this can be found in [Rueckauer et al. 2017](https://www.frontiersin.org/articles/10.3389/fnins.2017.00682/full)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_layers = [name for name, child in ann.named_children() if isinstance(child, (nn.Conv2d, nn.Linear))]\n",
    "output_layers = [name for name, child in ann.named_children() if isinstance(child, nn.ReLU)]\n",
    "output_layers += [param_layers[-1]]\n",
    "normalise_loader = DataLoader(dataset=test_dataset, batch_size=10, shuffle=True)\n",
    "sample_data = next(iter(normalise_loader))[0].flatten(0, 1)\n",
    "percentile = 99.99\n",
    "\n",
    "\n",
    "sinabs.utils.normalize_weights(ann, sample_data, output_layers=output_layers, param_layers=param_layers, percentile=percentile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are 5 parameter layers in the above defined model. You will see this come into play later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to Spiking CNN\n",
    "\n",
    "We can use the `from_torch` method from SINABS to convert our CNN to a SNN. The returned object contains the original CNN as `analog_model` and the newly generated SNN as `spiking_model`. The `ReLU`s have been converted to `SpikingLayers`. In addition, we have also added a spiking layer at the end. This is for compatibitlity with the chip later on, since the chips can only produce spikes and not  activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (1): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1)\n",
      "  (2): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (3): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "  (4): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1)\n",
      "  (5): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (6): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "  (7): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1)\n",
      "  (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=128, out_features=500, bias=False)\n",
      "  (11): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1)\n",
      "  (12): Linear(in_features=500, out_features=10, bias=False)\n",
      "  (Spiking output): IAFSqueeze(spike_threshold=1.0, min_v_mem=-1)\n",
      ")\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "sinabs_model = from_model(ann, add_spiking_output=True, min_v_mem=-1)\n",
    "print(sinabs_model.spiking_model)\n",
    "\n",
    "print(sinabs_model.spiking_model[1].min_v_mem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DYNAP-CNN compatible network\n",
    "\n",
    "The next step is to convert the SNN to a `DynapcnnNetwork`. This way we can be sure that all functionalities of our network are supported by the hardware and we can simulate the expected hardware output for testing purposes. This object will also generate the configuration objects to set up the chip.\n",
    "\n",
    "We need to tell the chip the dimensions of the input data. This can be done either by specifying an `input_shape` argument in the constructor or including a SINABS `InputLayer` at the beginning of the model.\n",
    "\n",
    "The class will convert the parameters (weights, biases, and thresholds) to discrete values that are supported by DYNAP-CNN. For testing purposes this can be disabled by setting `discretize` to `False`.\n",
    "\n",
    "We can use the `dvs_input` flag to determine whether the chip should process data coming from the on-chip dynamic vision sensor (DVS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DynapcnnNetwork(\n",
      "  (sequence): Sequential(\n",
      "    (0): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze(spike_threshold=795.0, min_v_mem=-795.0)\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (1): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze(spike_threshold=2122.0, min_v_mem=-2122.0)\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (2): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze(spike_threshold=2366.0, min_v_mem=-2366.0)\n",
      "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "    )\n",
      "    (3): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(128, 500, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze(spike_threshold=1613.0, min_v_mem=-1613.0)\n",
      "    )\n",
      "    (4): DynapcnnLayer(\n",
      "      (conv_layer): Conv2d(500, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (spk_layer): IAFSqueeze(spike_threshold=1005.0, min_v_mem=-1005.0)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# - Input dimensions\n",
    "input_shape = (1, 28, 28)\n",
    "\n",
    "# - DYNAP-CNN compatible network\n",
    "dynapcnn_net = DynapcnnNetwork(\n",
    "    sinabs_model.spiking_model,\n",
    "    input_shape=input_shape,\n",
    "    discretize=True,\n",
    "    dvs_input=False,\n",
    ")\n",
    "print(dynapcnn_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting model consists of 5 `DynapcnnLayer` objects, each containing a convolutional, a spiking, and possibly a pooling layer. Because there were 5 parameter layers (as we pointed out earlier), each gets mapped into a separate layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 99/100 [00:38<00:00,  2.58it/s, acc=99]  \n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    samples = 0\n",
    "    # we will only use the first 100 samples to save some time\n",
    "    pbar = tqdm(test_dataset, total=100)\n",
    "    for data, label in pbar:\n",
    "        dynapcnn_net.reset_states()\n",
    "        out = dynapcnn_net(data)\n",
    "\n",
    "        # Calculate total number of spikes out\n",
    "        pred = out.squeeze().sum(0)\n",
    "        \n",
    "        # Check if the prediction matches the label\n",
    "        if pred.argmax() == label:\n",
    "            correct += 1\n",
    "        samples += 1\n",
    "        pbar.set_postfix(acc=100*correct/samples)\n",
    "        if samples >= 100:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 1, 28, 28])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final accuracy of this model can now be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy of the  dynapcnn_net is: 99.0%'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"Accuracy of the  dynapcnn_net is: {100*correct/samples}%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Porting model to DYNAP-CNN DevKit\n",
    "\n",
    "Similar to porting a model to `cpu` or `gpu` in pytorch, the `DynapcnnNetwork` is a special class that supports porting a model to hardware based on DYNAP-CNN technology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DynapcnnNetwork(\n",
       "  (sequence): Sequential(\n",
       "    (0): DynapcnnLayer(\n",
       "      (conv_layer): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (spk_layer): IAFSqueeze(spike_threshold=795.0, min_v_mem=-795.0)\n",
       "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
       "    )\n",
       "    (1): DynapcnnLayer(\n",
       "      (conv_layer): Conv2d(20, 32, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
       "      (spk_layer): IAFSqueeze(spike_threshold=2122.0, min_v_mem=-2122.0)\n",
       "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
       "    )\n",
       "    (2): DynapcnnLayer(\n",
       "      (conv_layer): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
       "      (spk_layer): IAFSqueeze(spike_threshold=2366.0, min_v_mem=-2366.0)\n",
       "      (pool_layer): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
       "    )\n",
       "    (3): DynapcnnLayer(\n",
       "      (conv_layer): Conv2d(128, 500, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (spk_layer): IAFSqueeze(spike_threshold=1613.0, min_v_mem=-1613.0)\n",
       "    )\n",
       "    (4): DynapcnnLayer(\n",
       "      (conv_layer): Conv2d(500, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (spk_layer): IAFSqueeze(spike_threshold=1005.0, min_v_mem=-1005.0)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply model to device such as dynapcnndevkit, speck2, speck2b\n",
    "device = \"dynapcnndevkit:0\"\n",
    "dynapcnn_net.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can findout where the individual layers have been placed on the chip by looking at variable `chip_layers_ordering`. This information will come in handy when trying to send or receive events from the chip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 3, 5, 6, 1]\n"
     ]
    }
   ],
   "source": [
    "print(dynapcnn_net.chip_layers_ordering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inspecting memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the model described above, we were able to find a sequence of layers on the chip which could be utilized. If we had a larger model that does not fit the chip, the `to` method call would have raised a `ValueError`. In that case, it would be handy to understand the model's memory requirements so that it can be modified to fit the chip. A quick overview of the memory requirements of the model can be found by calling the method `memory_summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kernel': [1024.0, 20480.0, 65536.0, 65536.0, 8000.0],\n",
       " 'neuron': [20480.0, 2048.0, 512.0, 500.0, 10.0],\n",
       " 'bias': [0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dynapcnn_net.memory_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note that this memory is not the same as the total number of kernel parameters. See the `memory_summary` documentation for more details on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `memory_summary()` method returns the number of parameters/memory required for each layer of the  `DynapcnnNetwork` for kernel, neruons and bias. We see the corresponding values for the 5 layers of our current model above.\n",
    "\n",
    "You can compare these values to the `constrains` of the chip. For instance, the chip constraints for your device can be viewed from `DynapcnnConfigBuilder.get_constriants()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=16384, neuron_memory=65536, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=32768, neuron_memory=32768, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=32768, neuron_memory=32768, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=65536, neuron_memory=16384, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=65536, neuron_memory=16384, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=16384, neuron_memory=16384, bias_memory=1024),\n",
       " LayerConstraints(kernel_memory=16384, neuron_memory=16384, bias_memory=1024)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "builder = ChipFactory(device).get_config_builder()\n",
    "builder.get_constraints()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sending a receiving spikes\n",
    "\n",
    "Last but not least, we want to be able to send and receive spikes from the chips.\n",
    "\n",
    "So first we start by generating some spike events. Unlike simulations where the spikes were presented as a `tensor` (a raster of spikes), for the chip, we will send a sequence of custom event objects. We can convert a spike raster `tensor` to a list of events using the utility funciton `ChipFactory.raster_to_events()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "raster, label = test_dataset[0]\n",
    "\n",
    "factory = ChipFactory(device)\n",
    "first_layer_idx = dynapcnn_net.chip_layers_ordering[0] \n",
    "events_in = factory.raster_to_events(raster, layer=first_layer_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make sure that neuron states are reset to zero before sending these events to the chip. You can do this similarly to what you would do for a `pytorch` model on GPU or CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dynapcnn_net.reset_states()\n",
    "events_out = dynapcnn_net(events_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We receive spike objects from the hardware as output. We expect them to be from the last layer of the model (chip_layers_ordering[-1]) and for the data we sent in, we expect most spikes to be from the neuron index equal to label. Let's convert those to a tensor/raster format for easy slicing. Since the last layer has dimensions (10, 1, 1) for (feature, y, x), we are interested in the feature value of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  3.,   0.,  53.,  37.,   0.,   0.,   0., 164.,   0.,  30.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = factory.events_to_raster(events_out)\n",
    "output.sum([0,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see above that neuron 7 produced most spikes. Lets see what the original label of our data was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Success. Our model on the chip identified the input data correctly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
