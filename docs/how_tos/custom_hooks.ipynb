{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86c95b51-440d-4427-8a4e-bbe8000fe5c9",
   "metadata": {},
   "source": [
    "# Add custom hooks to monitor network properties\n",
    "\n",
    "As shown in [this how-to](synops_loss_snn.ipynb), Sinabs provides functions to monitor network activities such as synaptic operations and firing rates. For this it uses the hook-mechanism of PyTorch modules. This makes it easy to monitor custom statistics by writing our own hooks. In this how-to we will see how we can use this to keep track of the number of neurons and shapes of the spiking layers in our networks.\n",
    "\n",
    "## Setup and network definition\n",
    "\n",
    "Let's start by importing all necessary packages and by setting up a simple SNN in sinabs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517a4174-8090-41b0-b19c-f259cefd8ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, List\n",
    "import torch\n",
    "from torch import nn\n",
    "from sinabs import layers as sl\n",
    "import sinabs.hooks\n",
    "\n",
    "\n",
    "# - Define SNN\n",
    "class SNN(nn.Sequential):\n",
    "    def __init__(self, batch_size):\n",
    "        super().__init__(\n",
    "            sl.FlattenTime(),\n",
    "            nn.Conv2d(1, 16, 5, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.SumPool2d(2),\n",
    "            nn.Conv2d(16, 32, 5, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.SumPool2d(2),\n",
    "            nn.Conv2d(32, 120, 4, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(120, 10, bias=False),\n",
    "            sl.IAFSqueeze(batch_size=batch_size),\n",
    "            sl.UnflattenTime(batch_size=batch_size),\n",
    "        )\n",
    "\n",
    "\n",
    "batch_size = 5\n",
    "snn = SNN(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa77fa3-f38f-45df-b580-11e4e506d7e1",
   "metadata": {},
   "source": [
    "## Set up hook\n",
    "\n",
    "Now let's define a hook that captures the shape of our spiking layers. It will do so by looking at the shape of the output of the layers.\n",
    "\n",
    "In general, a hook is a function with the following signature:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e91ae9e-291b-46f2-bba0-33ff4ef64a5c",
   "metadata": {},
   "source": [
    "hook(module: nn.Module, input_: List[Any], output: Any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b1a877-33f9-48fa-97ca-16437b724b17",
   "metadata": {},
   "source": [
    "It has three parameters. The first is the module to which the hook is registered. The other two are a list of all inputs to the layer, as well as the output of the layer. If registered with a PyTorch module, the hook will always be executed after the forward method of that module is called.  \n",
    "\n",
    "### Hook definition\n",
    "Let's define our custom hook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c0d22f7-776a-4702-a8f6-0653e1834c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hook\n",
    "def monitor_shape(module: nn.Module, input_: List[Any], output: Any):\n",
    "    batch_size, *neuron_shape = output.shape\n",
    "    hook_data = sinabs.hooks.get_hook_data_dict(module)\n",
    "    hook_data[\"batch_size\"] = batch_size\n",
    "    hook_data[\"neuron_shape\"] = neuron_shape\n",
    "    hook_data[\"num_neurons\"] = output[0].numel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a660c7-42b9-45ae-b16d-509d9393bbb1",
   "metadata": {},
   "source": [
    "The hook calls the `get_hook_data_dict` function from the `sinabs.hooks` module. This is convenience function checks if the module already has an attribute `hook_data`. If so, it will return it. Otherwise it will create a dictionnary as that attribute and return it. \n",
    "\n",
    "We then extract the information we need from the layer output and write it into the `hook_data`. The layer input will be ignored in this case.\n",
    "\n",
    "Note that in principle the hook can do pretty much whatever it wants and we don't have to use a `hook_data` attribute. However, it is a nice convenience to have some consistency between different hooks. \n",
    "\n",
    "### Hook registration\n",
    "\n",
    "To register the hook we just need to call the `register_forward_hook` method that any Pytorch Module object (including all Sinabs layers) has. We could therefore register our hook with any layer of the network. However, in this case we are only interested in the spiking layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8705153-fe6b-4310-af79-c7b99f49a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register hooks\n",
    "for layer in snn:\n",
    "    if isinstance(layer, sl.IAFSqueeze):\n",
    "        layer.register_forward_hook(monitor_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afadefb-9c99-4284-a8d4-a8f67a9b18ee",
   "metadata": {},
   "source": [
    "Now our hook will be called automatically with each forward call of the spiking layers. There is no need to call this hook manually.\n",
    "The hook will then update the data in the `hook_data` dictionnary of these layers, where we can access it. Let's run some data through the network to see if everything works as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d439e387-5bd0-475e-ae25-d1efbdc3c9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 2:\n",
      "\tBatch size: 50\n",
      "\tShape: [16, 24, 24] - 9216 neurons in total\n",
      "Layer 5:\n",
      "\tBatch size: 50\n",
      "\tShape: [32, 8, 8] - 2048 neurons in total\n",
      "Layer 8:\n",
      "\tBatch size: 50\n",
      "\tShape: [120, 1, 1] - 120 neurons in total\n",
      "Layer 11:\n",
      "\tBatch size: 50\n",
      "\tShape: [10] - 10 neurons in total\n"
     ]
    }
   ],
   "source": [
    "# Forward pass\n",
    "rand_input_spikes = (torch.ones((batch_size, 10, 1, 28, 28))).float()\n",
    "snn(rand_input_spikes)\n",
    "\n",
    "# Access and print hook data\n",
    "for idx, layer in enumerate(snn):\n",
    "    if hasattr(layer, \"hook_data\"):\n",
    "        print(f\"Layer {idx}:\")\n",
    "        print(f\"\\tBatch size: {layer.hook_data['batch_size']}\")\n",
    "        print(\n",
    "            f\"\\tShape: {layer.hook_data['neuron_shape']} - {layer.hook_data['num_neurons']} neurons in total\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ccaa1e4-d438-40d6-82b7-cf9ef578443c",
   "metadata": {},
   "source": [
    "You might be surprised that the batch size is recorded as `50` and not `5` as defind above. This is because in Sinabs the batch and time dimensions are usually flattened out, so that the shape is compatible with the 2D operations of PyTorch, such as `Conv2D`. This is done by the `Flatten` layer in the beginning, and undone by the `Unflatten` layer in the end of the networks. Within the network, batch and time dimensions are only separated internally within the `IAFSqueeze` layers. Therefore to the hook it seems like the batch size is multiplied by the number of timesteps.\n",
    "\n",
    "## Are there backward hooks?\n",
    "You may have noticed that we are using forward hooks in this example and might be wondering whether there are also backward hooks. The answer is yes, Sinabs layers are essentially Pytorch Modules and therefore support both forward and backward hooks. To learn more about hooks, you can also check out the [PyTorch docs](https://pytorch.org/tutorials/beginner/former_torchies/nnft_tutorial.html#forward-and-backward-function-hooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bddea6f-665e-41b7-a954-d5febcd7d68b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
