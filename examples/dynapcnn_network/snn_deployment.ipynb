{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samurai2077/anaconda3/envs/speck-rescnn/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from sinabs.layers import Merge, IAFSqueeze, SumPool2d\n",
    "from sinabs.activation.surrogate_gradient_fn import PeriodicExponential\n",
    "import sinabs.layers as sl\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tonic.datasets.nmnist import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x76c1f74d9070>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Module\n",
    "\n",
    "We need to define a `nn.Module` implementing the Spiking Neural Network (SNN) we want to deploy on chip. The configuration of the network on the chip needs to know in advance the shape of the input data and the batch size that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 2\n",
    "height = 34\n",
    "width = 34\n",
    "batch_size = 8\n",
    "\n",
    "input_shape = (channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # -- chip core A --\n",
    "        self.conv1 = nn.Conv2d(2, 10, 2, 1, bias=False)\n",
    "        self.iaf1 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        self.pool1 = nn.AvgPool2d(2,2)\n",
    "        # -- chip core B --\n",
    "        self.conv2 = nn.Conv2d(10, 10, 4, 1, bias=False)\n",
    "        self.iaf2 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core C --\n",
    "        self.conv3 = nn.Conv2d(10, 1, 2, 1, bias=False)\n",
    "        self.iaf3 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core D --\n",
    "        self.fc1 = nn.Linear(144, 200, bias=False)\n",
    "        self.iaf4 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core E --\n",
    "        self.fc2 = nn.Linear(200, 10, bias=False)\n",
    "        self.iaf5 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "\n",
    "        # -- layers ignored during deployment --\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "    def detach_neuron_states(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if name != '':\n",
    "                if isinstance(layer, sl.StatefulLayer):\n",
    "                    for name, buffer in layer.named_buffers():\n",
    "                        buffer.detach_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        con1_out = self.conv1(x)\n",
    "        iaf1_out = self.iaf1(con1_out)\n",
    "        pool1_out = self.pool1(iaf1_out)\n",
    "\n",
    "        conv2_out = self.conv2(pool1_out)\n",
    "        iaf2_out = self.iaf2(conv2_out)\n",
    "\n",
    "        conv3_out = self.conv3(iaf2_out)\n",
    "        iaf3_out = self.iaf3(conv3_out)\n",
    "\n",
    "        flat_out = self.flat(iaf3_out)\n",
    "        \n",
    "        fc1_out = self.fc1(flat_out)\n",
    "        iaf4_out = self.iaf4(fc1_out)\n",
    "        fc2_out = self.fc2(iaf4_out)\n",
    "        iaf5_out = self.iaf5(fc2_out)\n",
    "\n",
    "        return iaf5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "snn = SNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model to see what kind of accuracy the software model gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/1afc103f-8799-464a-a214-81bb9b1f9337 to ./NMNIST/NMNIST/train.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1011894272it [01:25, 11770103.44it/s]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./NMNIST/NMNIST/train.zip to ./NMNIST/NMNIST\n",
      "Downloading https://prod-dcd-datasets-public-files-eu-west-1.s3.eu-west-1.amazonaws.com/a99d0fee-a95b-4231-ad22-988fdb0a2411 to ./NMNIST/NMNIST/test.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "169675776it [00:18, 9035099.75it/s]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./NMNIST/NMNIST/test.zip to ./NMNIST/NMNIST\n",
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (50, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "_ = NMNIST(save_to='./NMNIST', train=True)\n",
    "_ = NMNIST(save_to='./NMNIST', train=False)\n",
    "\n",
    "nb_time_steps = 50\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=nb_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to='./NMNIST', train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to='./NMNIST', train=False, transform=to_raster)\n",
    "\n",
    "sample_data, label = snn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [i for i in range(1000)]\n",
    "test_indices = [i for i in range(100)]\n",
    "\n",
    "snn_train_dataset_subset = torch.utils.data.Subset(snn_train_dataset, train_indices)\n",
    "snn_test_subset = torch.utils.data.Subset(snn_train_dataset, test_indices)\n",
    "\n",
    "snn_train_dataloader = DataLoader(snn_train_dataset_subset, batch_size=batch_size, num_workers=4, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_subset, batch_size=batch_size, num_workers=4, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (conv1): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "  (iaf1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  (iaf2): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (conv3): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "  (iaf3): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (fc1): Linear(in_features=144, out_features=200, bias=False)\n",
       "  (iaf4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (fc2): Linear(in_features=200, out_features=10, bias=False)\n",
       "  (iaf5): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "snn.init_weights()\n",
    "\n",
    "snn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(snn.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5579aac6828434dbac67d04236a87c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snn.train()\n",
    "for e in range(1):\n",
    "    train_p_bar = tqdm(snn_train_dataloader, total=int(len(snn_train_dataset_subset)/batch_size))\n",
    "\n",
    "    for X, y in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        X = X.reshape(-1, NMNIST.sensor_size[2], NMNIST.sensor_size[0], NMNIST.sensor_size[1]).to(dtype=torch.float, device=device)\n",
    "        y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "        # forward\n",
    "        pred = snn(X)\n",
    "\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        pred = pred.reshape(batch_size, nb_time_steps, -1)\n",
    "\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        pred = pred.sum(dim = 1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        snn.detach_neuron_states()\n",
    "\n",
    "        train_p_bar.set_description(f\"epoch {e} - BPTT training loss: {round(loss.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da70870c954cac9529464f72f8887b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = []\n",
    "\n",
    "snn.eval()\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader, total=int(len(snn_test_dataloader)/batch_size))\n",
    "    \n",
    "    for X, y in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        X = X.reshape(-1, NMNIST.sensor_size[2], NMNIST.sensor_size[0], NMNIST.sensor_size[1]).to(dtype=torch.float, device=device)\n",
    "        y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "        # forward\n",
    "        output = snn(X)\n",
    "\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, nb_time_steps, -1)\n",
    "\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(y.view_as(pred)))\n",
    "\n",
    "        test_p_bar.set_description(f\"Testing Model...\")\n",
    "\n",
    "correct_predictions = torch.cat(correct_predictions)\n",
    "test_acc = correct_predictions.sum().item()/(len(correct_predictions))*100\n",
    "\n",
    "print(f'test accuracy: {round(test_acc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the Model: Enter the DynapcnnNetwork Class\n",
    "\n",
    "In the constructor of `DynapcnnNetworkGraph` the SNN passed as argument (defined as a `nn.Module`) will be parsed such that each layer is represented in a computational graph (using `nirtorch.extract_torch_graph`). \n",
    "\n",
    "The layers are the `nodes` of the graph, while their connectivity (how the outputs from a layer are sent to other layers) is represented as `edges`, represented in a `list` of `tuples`.\n",
    "\n",
    "Once the constructor finishes its initialization, the `hw_model.dynapcnn_layers` property is a dictionary where each entry represents the ID of a `DynapcnnLayer` instance (an `int` from `0` to `L`), with this entry containing a `DynapcnnLayer` instance where a subset of the layers in the original SNN has been incorporated into, the core such instance has been assigned to, and the list of `DynapcnnLayer` instances (their IDs) the layer targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "randn() received an invalid combination of arguments - got (tuple, bool), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hw_model \u001b[38;5;241m=\u001b[39m \u001b[43mDynapcnnNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43msnn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msnn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscretize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Github/sinabs/sinabs/backend/dynapcnn/dynapcnn_network.py:70\u001b[0m, in \u001b[0;36mDynapcnnNetwork.__init__\u001b[0;34m(self, snn, input_shape, batch_size, dvs_input, discretize, weight_rescaling_fn)\u001b[0m\n\u001b[1;32m     67\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m sinabs\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mget_smallest_compatible_time_dimension(snn)\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# computational graph from original PyTorch module.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_extractor \u001b[38;5;241m=\u001b[39m GraphExtractor(\n\u001b[0;32m---> 70\u001b[0m     snn, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdvs_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m )  \u001b[38;5;66;03m# needs the batch dimension.\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# Remove nodes of ignored classes (including merge nodes)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_extractor\u001b[38;5;241m.\u001b[39mremove_nodes_by_class(DEFAULT_IGNORED_LAYER_TYPES)\n",
      "\u001b[0;31mTypeError\u001b[0m: randn() received an invalid combination of arguments - got (tuple, bool), but expected one of:\n * (tuple of ints size, *, torch.Generator generator, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, torch.Generator generator, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, Tensor out, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n * (tuple of ints size, *, tuple of names names, torch.dtype dtype, torch.layout layout, torch.device device, bool pin_memory, bool requires_grad)\n"
     ]
    }
   ],
   "source": [
    "hw_model = DynapcnnNetwork(\n",
    "    snn=snn,\n",
    "    input_shape=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    discretize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the model bellow how the property <code>DynapcnnLayer</code> in the model has yet to be assigned to a core. This is only done once\n",
    "<code>DynapcnnNetworkGraph.to()</code> is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- [ DynapcnnLayer 0 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 0): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 1): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(241.), min_v_mem=Parameter containing:\n",
      "tensor(-241.), batch_size=8, num_timesteps=-1)\n",
      "(node 2): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: True\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [1]\n",
      "> node 2 feeds input to nodes [3]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 1 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 3): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "(node 4): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1041.), min_v_mem=Parameter containing:\n",
      "tensor(-1041.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: 2.0\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [2]\n",
      "> node 4 feeds input to nodes [5]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 2 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 5): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 6): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(247.), min_v_mem=Parameter containing:\n",
      "tensor(-247.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [3]\n",
      "> node 6 feeds input to nodes [7]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 3 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 7): Conv2d(1, 200, kernel_size=(12, 12), stride=(1, 1), bias=False)\n",
      "(node 8): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(363.), min_v_mem=Parameter containing:\n",
      "tensor(-363.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [4]\n",
      "> node 8 feeds input to nodes [9]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 4 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 9): Conv2d(200, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "(node 10): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(422.), min_v_mem=Parameter containing:\n",
      "tensor(-422.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hw_model.to()` call will figure out into which core each `DynapcnnLayer` instance will be assigned to. Once this assingment is made the instance itself is used to configure the `CNNLayerConfig` instance representing the core's configuration assigned to it.\n",
    "\n",
    "If the call is sucessfull, the layers comprising the network and their associated metadata will be printed. To deploy the model, we need to provide the device string defining what Speck devkit is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "speck_device = \"speck2fmodule:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DynapcnnNetwork()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_model.to(device=speck_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- [ DynapcnnLayer 0 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 0): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 1): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(241.), min_v_mem=Parameter containing:\n",
      "tensor(-241.), batch_size=8, num_timesteps=-1)\n",
      "(node 2): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: True\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 0\n",
      "> destination DynapcnnLayers: [1]\n",
      "> node 2 feeds input to nodes [3]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 1 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 3): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "(node 4): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1041.), min_v_mem=Parameter containing:\n",
      "tensor(-1041.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: 2.0\n",
      "> assigned core index: 1\n",
      "> destination DynapcnnLayers: [2]\n",
      "> node 4 feeds input to nodes [5]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 2 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 5): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 6): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(247.), min_v_mem=Parameter containing:\n",
      "tensor(-247.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 2\n",
      "> destination DynapcnnLayers: [3]\n",
      "> node 6 feeds input to nodes [7]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 3 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 7): Conv2d(1, 200, kernel_size=(12, 12), stride=(1, 1), bias=False)\n",
      "(node 8): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(363.), min_v_mem=Parameter containing:\n",
      "tensor(-363.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 5\n",
      "> destination DynapcnnLayers: [4]\n",
      "> node 8 feeds input to nodes [9]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 4 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 9): Conv2d(200, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "(node 10): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(422.), min_v_mem=Parameter containing:\n",
      "tensor(-422.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 3\n",
      "> destination DynapcnnLayers: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikes IN/Out of the Chip\n",
    "\n",
    "Let's try to use our network configured on the chip to forward some data. We'll get a sample from the NMNIST dataset to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dataset = NMNIST(save_to='./NMNIST', train=False)\n",
    "event_subset = torch.utils.data.Subset(event_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(event_dataset.targets)\n",
    "target_indices = {idx: np.where(targets == idx)[0] for idx in range(10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with data and want to convert it to <code>input_events</code>, you would instantiate a <code>ChipFactory</code> object providing the device string (<code>\"speck2fsomethingsomething\"</code>) as instantiation argument. For further details consult the [documentation](https://sinabs.readthedocs.io/en/v2.0.0/tutorials/nir_to_speck.html#prepare-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.backend.dynapcnn.chip_factory import ChipFactory\n",
    "\n",
    "chip_factory = ChipFactory(speck_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has a method <code>raster_to_events</code> (see more [here](https://sinabs.readthedocs.io/en/v2.0.0/speck/api/dynapcnn/chip_factory.html#sinabs.backend.dynapcnn.chip_factory.ChipFactory.raster_to_events)) that can convert your data to an event list, which is what the chip expects. This method requires a 4 dimensional tensor of spike events with the dimensions <code>[Time, Channel, Height, Width]</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output core id: 3\n",
      "input core id: 0\n"
     ]
    }
   ],
   "source": [
    "layer_out = hw_model.get_output_core_id()                   # core assigned to the output layer of the model\n",
    "layer_in = hw_model.get_input_core_id()[-1]                 # core assigned to the input layyer of the model\n",
    "\n",
    "print(f'output core id: {layer_out}')\n",
    "print(f'input core id: {layer_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer monitoring: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output layer monitoring: {hw_model.samna_config.cnn_layers[layer_out].monitor_enable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e50af00044f1ab9625886fe3f36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on speck: 100.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "predictions = []\n",
    "\n",
    "for (sample, target) in tqdm(event_subset, total=len(event_subset)):\n",
    "    input_events = chip_factory.xytp_to_events(sample, layer=layer_in, reset_timestamps=True)\n",
    "    output = hw_model.hw_forward(input_events)\n",
    "    prediction = mode((event.feature for event in output)) if output else -1\n",
    "    correct += (prediction == target)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "accuracy = correct / len(event_subset)\n",
    "print(f\"Test accuracy on speck: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck-rescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
