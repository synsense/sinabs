{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sinabs.backend.dynapcnn import DynapcnnNetwork\n",
    "from sinabs.layers import Merge, IAFSqueeze, SumPool2d\n",
    "from sinabs.activation.surrogate_gradient_fn import PeriodicExponential\n",
    "import sinabs.layers as sl\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tonic.datasets.nmnist import NMNIST\n",
    "from tonic.transforms import ToFrame\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from statistics import mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f87e695c5b0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Module\n",
    "\n",
    "We need to define a `nn.Module` implementing the Spiking Neural Network (SNN) we want to deploy on chip. The configuration of the network on the chip needs to know in advance the shape of the input data and the batch size that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = 2\n",
    "height = 34\n",
    "width = 34\n",
    "batch_size = 8\n",
    "\n",
    "input_shape = (channels, height, width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class SNN(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        # -- chip core A --\n",
    "        self.conv1 = nn.Conv2d(2, 10, 2, 1, bias=False)\n",
    "        self.iaf1 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        self.pool1 = nn.AvgPool2d(2,2)\n",
    "        # -- chip core B --\n",
    "        self.conv2 = nn.Conv2d(10, 10, 4, 1, bias=False)\n",
    "        self.iaf2 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core C --\n",
    "        self.conv3 = nn.Conv2d(10, 1, 2, 1, bias=False)\n",
    "        self.iaf3 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core D --\n",
    "        self.fc1 = nn.Linear(144, 200, bias=False)\n",
    "        self.iaf4 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "        # -- chip core E --\n",
    "        self.fc2 = nn.Linear(200, 10, bias=False)\n",
    "        self.iaf5 = IAFSqueeze(batch_size=batch_size, min_v_mem=-1.0, spike_threshold=1.0, surrogate_grad_fn=PeriodicExponential())\n",
    "\n",
    "        # -- layers ignored during deployment --\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if isinstance(layer, nn.Conv2d) or isinstance(layer, nn.Linear):\n",
    "                nn.init.xavier_normal_(layer.weight.data)\n",
    "\n",
    "    def detach_neuron_states(self):\n",
    "        for name, layer in self.named_modules():\n",
    "            if name != '':\n",
    "                if isinstance(layer, sl.StatefulLayer):\n",
    "                    for name, buffer in layer.named_buffers():\n",
    "                        buffer.detach_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        con1_out = self.conv1(x)\n",
    "        iaf1_out = self.iaf1(con1_out)\n",
    "        pool1_out = self.pool1(iaf1_out)\n",
    "\n",
    "        conv2_out = self.conv2(pool1_out)\n",
    "        iaf2_out = self.iaf2(conv2_out)\n",
    "\n",
    "        conv3_out = self.conv3(iaf2_out)\n",
    "        iaf3_out = self.iaf3(conv3_out)\n",
    "\n",
    "        flat_out = self.flat(iaf3_out)\n",
    "        \n",
    "        fc1_out = self.fc1(flat_out)\n",
    "        iaf4_out = self.iaf4(fc1_out)\n",
    "        fc2_out = self.fc2(iaf4_out)\n",
    "        iaf5_out = self.iaf5(fc2_out)\n",
    "\n",
    "        return iaf5_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "snn = SNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model to see what kind of accuracy the software model gets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The transformed array is in shape [Time-Step, Channel, Height, Width] --> (50, 2, 34, 34)\n"
     ]
    }
   ],
   "source": [
    "_ = NMNIST(save_to='./NMNIST', train=True)\n",
    "_ = NMNIST(save_to='./NMNIST', train=False)\n",
    "\n",
    "nb_time_steps = 50\n",
    "to_raster = ToFrame(sensor_size=NMNIST.sensor_size, n_time_bins=nb_time_steps)\n",
    "\n",
    "snn_train_dataset = NMNIST(save_to='./NMNIST', train=True, transform=to_raster)\n",
    "snn_test_dataset = NMNIST(save_to='./NMNIST', train=False, transform=to_raster)\n",
    "\n",
    "sample_data, label = snn_train_dataset[0]\n",
    "print(f\"The transformed array is in shape [Time-Step, Channel, Height, Width] --> {sample_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = [i for i in range(1000)]\n",
    "test_indices = [i for i in range(100)]\n",
    "\n",
    "snn_train_dataset_subset = torch.utils.data.Subset(snn_train_dataset, train_indices)\n",
    "snn_test_subset = torch.utils.data.Subset(snn_train_dataset, test_indices)\n",
    "\n",
    "snn_train_dataloader = DataLoader(snn_train_dataset_subset, batch_size=batch_size, num_workers=4, drop_last=True, shuffle=True)\n",
    "snn_test_dataloader = DataLoader(snn_test_subset, batch_size=batch_size, num_workers=4, drop_last=True, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SNN(\n",
       "  (conv1): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "  (iaf1): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (pool1): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "  (conv2): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  (iaf2): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (conv3): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
       "  (iaf3): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (fc1): Linear(in_features=144, out_features=200, bias=False)\n",
       "  (iaf4): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (fc2): Linear(in_features=200, out_features=10, bias=False)\n",
       "  (iaf5): IAFSqueeze(spike_threshold=Parameter containing:\n",
       "  tensor(1.), min_v_mem=Parameter containing:\n",
       "  tensor(-1.), batch_size=8, num_timesteps=-1)\n",
       "  (flat): Flatten(start_dim=1, end_dim=-1)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "snn.init_weights()\n",
    "\n",
    "snn.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(snn.parameters(), lr=1e-4, betas=(0.9, 0.999), eps=1e-8)\n",
    "loss_fn = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5579aac6828434dbac67d04236a87c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "snn.train()\n",
    "for e in range(1):\n",
    "    train_p_bar = tqdm(snn_train_dataloader, total=int(len(snn_train_dataset_subset)/batch_size))\n",
    "\n",
    "    for X, y in train_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        X = X.reshape(-1, NMNIST.sensor_size[2], NMNIST.sensor_size[0], NMNIST.sensor_size[1]).to(dtype=torch.float, device=device)\n",
    "        y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "        # forward\n",
    "        pred = snn(X)\n",
    "\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        pred = pred.reshape(batch_size, nb_time_steps, -1)\n",
    "\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        pred = pred.sum(dim = 1)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # gradient update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # detach the neuron states and activations from current computation graph(necessary)\n",
    "        snn.detach_neuron_states()\n",
    "\n",
    "        train_p_bar.set_description(f\"epoch {e} - BPTT training loss: {round(loss.item(), 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing the model..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68da70870c954cac9529464f72f8887b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = []\n",
    "\n",
    "snn.eval()\n",
    "with torch.no_grad():\n",
    "    test_p_bar = tqdm(snn_test_dataloader, total=int(len(snn_test_dataloader)/batch_size))\n",
    "    \n",
    "    for X, y in test_p_bar:\n",
    "        # reshape the input from [Batch, Time, Channel, Height, Width] into [Batch*Time, Channel, Height, Width]\n",
    "        X = X.reshape(-1, NMNIST.sensor_size[2], NMNIST.sensor_size[0], NMNIST.sensor_size[1]).to(dtype=torch.float, device=device)\n",
    "        y = y.to(dtype=torch.long, device=device)\n",
    "\n",
    "        # forward\n",
    "        output = snn(X)\n",
    "\n",
    "        # reshape the output from [Batch*Time,num_classes] into [Batch, Time, num_classes]\n",
    "        output = output.reshape(batch_size, nb_time_steps, -1)\n",
    "\n",
    "        # accumulate all time-steps output for final prediction\n",
    "        output = output.sum(dim=1)\n",
    "\n",
    "        # calculate accuracy\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "\n",
    "        # compute the total correct predictions\n",
    "        correct_predictions.append(pred.eq(y.view_as(pred)))\n",
    "\n",
    "        test_p_bar.set_description(f\"Testing Model...\")\n",
    "\n",
    "correct_predictions = torch.cat(correct_predictions)\n",
    "test_acc = correct_predictions.sum().item()/(len(correct_predictions))*100\n",
    "\n",
    "print(f'test accuracy: {round(test_acc, 2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploying the Model: Enter the DynapcnnNetwork Class\n",
    "\n",
    "In the constructor of `DynapcnnNetworkGraph` the SNN passed as argument (defined as a `nn.Module`) will be parsed such that each layer is represented in a computational graph (using `nirtorch.extract_torch_graph`). \n",
    "\n",
    "The layers are the `nodes` of the graph, while their connectivity (how the outputs from a layer are sent to other layers) is represented as `edges`, represented in a `list` of `tuples`.\n",
    "\n",
    "Once the constructor finishes its initialization, the `hw_model.dynapcnn_layers` property is a dictionary where each entry represents the ID of a `DynapcnnLayer` instance (an `int` from `0` to `L`), with this entry containing a `DynapcnnLayer` instance where a subset of the layers in the original SNN has been incorporated into, the core such instance has been assigned to, and the list of `DynapcnnLayer` instances (their IDs) the layer targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "hw_model = DynapcnnNetwork(\n",
    "    snn=snn,\n",
    "    input_shape=input_shape,\n",
    "    batch_size=batch_size,\n",
    "    discretize=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice in the model bellow how the property <code>DynapcnnLayer</code> in the model has yet to be assigned to a core. This is only done once\n",
    "<code>DynapcnnNetworkGraph.to()</code> is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- [ DynapcnnLayer 0 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 0): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 1): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(241.), min_v_mem=Parameter containing:\n",
      "tensor(-241.), batch_size=8, num_timesteps=-1)\n",
      "(node 2): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: True\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [1]\n",
      "> node 2 feeds input to nodes [3]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 1 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 3): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "(node 4): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1041.), min_v_mem=Parameter containing:\n",
      "tensor(-1041.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: 2.0\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [2]\n",
      "> node 4 feeds input to nodes [5]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 2 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 5): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 6): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(247.), min_v_mem=Parameter containing:\n",
      "tensor(-247.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [3]\n",
      "> node 6 feeds input to nodes [7]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 3 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 7): Conv2d(1, 200, kernel_size=(12, 12), stride=(1, 1), bias=False)\n",
      "(node 8): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(363.), min_v_mem=Parameter containing:\n",
      "tensor(-363.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: [4]\n",
      "> node 8 feeds input to nodes [9]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 4 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 9): Conv2d(200, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "(node 10): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(422.), min_v_mem=Parameter containing:\n",
      "tensor(-422.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: None\n",
      "> destination DynapcnnLayers: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `hw_model.to()` call will figure out into which core each `DynapcnnLayer` instance will be assigned to. Once this assingment is made the instance itself is used to configure the `CNNLayerConfig` instance representing the core's configuration assigned to it.\n",
    "\n",
    "If the call is sucessfull, the layers comprising the network and their associated metadata will be printed. To deploy the model, we need to provide the device string defining what Speck devkit is being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "speck_device = \"speck2fmodule:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network is valid: \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DynapcnnNetwork()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hw_model.to(device=speck_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------- [ DynapcnnLayer 0 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 0): Conv2d(2, 10, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 1): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(241.), min_v_mem=Parameter containing:\n",
      "tensor(-241.), batch_size=8, num_timesteps=-1)\n",
      "(node 2): SumPool2d(norm_type=1, kernel_size=(2, 2), stride=None, ceil_mode=False)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: True\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 0\n",
      "> destination DynapcnnLayers: [1]\n",
      "> node 2 feeds input to nodes [3]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 1 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 3): Conv2d(10, 10, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "(node 4): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(1041.), min_v_mem=Parameter containing:\n",
      "tensor(-1041.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: 2.0\n",
      "> assigned core index: 1\n",
      "> destination DynapcnnLayers: [2]\n",
      "> node 4 feeds input to nodes [5]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 2 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 5): Conv2d(10, 1, kernel_size=(2, 2), stride=(1, 1), bias=False)\n",
      "(node 6): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(247.), min_v_mem=Parameter containing:\n",
      "tensor(-247.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 2\n",
      "> destination DynapcnnLayers: [3]\n",
      "> node 6 feeds input to nodes [7]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 3 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 7): Conv2d(1, 200, kernel_size=(12, 12), stride=(1, 1), bias=False)\n",
      "(node 8): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(363.), min_v_mem=Parameter containing:\n",
      "tensor(-363.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 5\n",
      "> destination DynapcnnLayers: [4]\n",
      "> node 8 feeds input to nodes [9]\n",
      "\n",
      "----------------------- [ DynapcnnLayer 4 ] -----------------------\n",
      "\n",
      "COMPUTATIONAL NODES:\n",
      "\n",
      "(node 9): Conv2d(200, 10, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "(node 10): IAFSqueeze(spike_threshold=Parameter containing:\n",
      "tensor(422.), min_v_mem=Parameter containing:\n",
      "tensor(-422.), batch_size=8, num_timesteps=-1)\n",
      "\n",
      "METADATA:\n",
      "\n",
      "> network's entry point: False\n",
      "> convolution's weight re-scaling factor: None\n",
      "> assigned core index: 3\n",
      "> destination DynapcnnLayers: []\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(hw_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spikes IN/Out of the Chip\n",
    "\n",
    "Let's try to use our network configured on the chip to forward some data. We'll get a sample from the NMNIST dataset to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_dataset = NMNIST(save_to='./NMNIST', train=False)\n",
    "event_subset = torch.utils.data.Subset(event_dataset, test_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = np.array(event_dataset.targets)\n",
    "target_indices = {idx: np.where(targets == idx)[0] for idx in range(10)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a tensor with data and want to convert it to <code>input_events</code>, you would instantiate a <code>ChipFactory</code> object providing the device string (<code>\"speck2fsomethingsomething\"</code>) as instantiation argument. For further details consult the [documentation](https://sinabs.readthedocs.io/en/v2.0.0/tutorials/nir_to_speck.html#prepare-dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sinabs.backend.dynapcnn.chip_factory import ChipFactory\n",
    "\n",
    "chip_factory = ChipFactory(speck_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This object has a method <code>raster_to_events</code> (see more [here](https://sinabs.readthedocs.io/en/v2.0.0/speck/api/dynapcnn/chip_factory.html#sinabs.backend.dynapcnn.chip_factory.ChipFactory.raster_to_events)) that can convert your data to an event list, which is what the chip expects. This method requires a 4 dimensional tensor of spike events with the dimensions <code>[Time, Channel, Height, Width]</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output core id: 3\n",
      "input core id: 0\n"
     ]
    }
   ],
   "source": [
    "layer_out = hw_model.get_output_core_id()                   # core assigned to the output layer of the model\n",
    "layer_in = hw_model.get_input_core_id()[-1]                 # core assigned to the input layyer of the model\n",
    "\n",
    "print(f'output core id: {layer_out}')\n",
    "print(f'input core id: {layer_in}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output layer monitoring: True\n"
     ]
    }
   ],
   "source": [
    "print(f'Output layer monitoring: {hw_model.samna_config.cnn_layers[layer_out].monitor_enable}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "646e50af00044f1ab9625886fe3f36ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy on speck: 100.00%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "predictions = []\n",
    "\n",
    "for (sample, target) in tqdm(event_subset, total=len(event_subset)):\n",
    "    input_events = chip_factory.xytp_to_events(sample, layer=layer_in, reset_timestamps=True)\n",
    "    output = hw_model.hw_forward(input_events)\n",
    "    prediction = mode((event.feature for event in output)) if output else -1\n",
    "    correct += (prediction == target)\n",
    "    predictions.append(prediction)\n",
    "\n",
    "accuracy = correct / len(event_subset)\n",
    "print(f\"Test accuracy on speck: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speck-rescnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
